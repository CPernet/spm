function [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
% active inference and learning using belief propagation (factorised)
% FORMAT [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
%
% Input; MDP(m,n)       - structure array of m models over n epochs
% MDP.U(P,F)            - P policies over F factors
% MDP.T                 - number of outcomes
%
% MDP.A{G}(O,N1,...,NF) - likelihood of O outcomes for modality G, given hidden states
% MDP.B{F}(N,N,U)       - transitions among N states under U control states
% MDP.C{G}(O)           - prior probabilities over final outcomes (log preferences)
% MDP.D{F}(N,1)         - prior probabilities over initial states (Dirichlet counts)
% MDP.E(P,1)            - prior probabilities over control states (Dirichlet counts)
%
% MDP.a{G}              - concentration parameters for A
% MDP.b{F}              - concentration parameters for B
% MDP.c{G}              - concentration parameters for C
% MDP.d{F}              - concentration parameters for D
% MDP.e{P}              - concentration parameters for E
%
% optional:
% MDP.s(F,T)            - matrix of true states - for each hidden factor
% MDP.o(G,T)            - matrix of outcomes    - for each outcome modality
% or .O{G,T}            - likelihoods           - for each outcome modality
% MDP.u(F,T - 1)        - vector of actions     - for each hidden factor
%
% MDP.alpha             - precision - action selection [512]
% MDP.chi               - Occams window for deep updates
% MDP.eta               - learning rate for model parameters
% MDP.N                 - depth of deep policy search [N <= T]
%
% MDP.demi.C            - Mixed model: cell array of true causes (DEM.C)
% MDP.demi.U            - Bayesian model average (DEM.U) see: spm_MDP_DEM
% MDP.link              - link array to generate outcomes from
%                         subordinate MDP; for deep (hierarchical) models
%
% MDP.n(O,T)            - outputs for modality O at time T are generated by
%                         agent n(O,T); unless n(O,T) = 0, when outputs
%                         our generated by the agents states
% MDP.m(F)              - states for factor F are generated for agent m(F);
%                         unless m(F) = 0, when states are updated for the
%                         agent in question
%
% OPTIONS.plot          - switch to suppress graphics:  (default: [0])
% OPTIONS.D             - switch to update initial states over epochs
% OPTIONS.BMR           - Bayesian model reduction for multiple trials
%                         see: spm_MDP_VB_sleep(MDP,BMR)
% Outputs:
%
% MDP.P{F}(U,T)         - conditional expectations over control states
% MDP.X{F}(N,T)         - conditional expectations over hidden states
% MDP.Y{O,T}            - conditional expectations over outcomes
% MDP.R(P,T)            - conditional expectations over policies
% MDP.r(1,T)            - selected policy
%
% MDP.F(1,T)            - (negative) free energies (states)  over time
% MDP.Z{U,T}            - (negative) free energies (control) over time
% MDP.G{P,T}            - (negative) expected free energies  over time
% MDP.Fa                - (negative) free energy of parameters (a)
% MDP.Fb                - ...
%
% MDP.v                 - expected free energy  over policies
% MDP.w                 - precision of beliefs about policies
% MDP.un                - simulated neuronal encoding of hidden states
% MDP.xn                - simulated neuronal encoding of policies
% MDP.wn                - simulated neuronal encoding of precision (tonic)
% MDP.dn                - simulated dopamine responses (phasic)
%
% This routine provides solutions of active inference (minimisation of
% variational free energy) using a generative model based upon a Markov
% decision process. The model and inference scheme is formulated in
% discrete space and time. This means that the generative model (and
% process) are hidden Markov models whose dynamics are given by transition
% probabilities among states and the likelihood corresponds to a particular
% outcome conditioned upon hidden states.
%
% This implementation equips agents with the prior beliefs that they will
% maximise expected free energy. Variational free energy can be interpreted
% in several ways - most intuitively as minimising the KL divergence
% between predicted and preferred outcomes (specified as prior beliefs) -
% while simultaneously minimising ambiguity.
%
% This particular scheme is designed for any allowable policies or control
% variables specified in MDP.U. Constraints on allowable policies can limit
% the numerics or combinatorics considerably. Further, the outcome space
% and hidden states can be defined in terms of factors; corresponding to
% sensory modalities and (functionally) segregated representations,
% respectively. This means, for each factor or subset of hidden states
% there are corresponding control states that determine the transition
% probabilities. in this implementation, hidden factors are combined using
% a Kronecker intensive product to enable exact Bayesian inference using
% belief propagation (the Kronecker tensor form ensures that conditional
% dependencies among hidden factors are evaluated).
%
% In this belief propagation scheme, the next action is evaluated in terms
% of the free energy expected under all subsequent actions until some time
% horizon (specified by MDP.T). This expected free energy is accumulated
% along all allowable paths or policies (see the subroutine spm_forward);
% effectively, performing a deep tree search over future sequences of
% actions. Because actions are conditionally independent of previous
% actions, it is only necessary to update posterior beliefs over hidden
% states at the current time point (using a Bayesian belief updating) and
% then use the prior over actions (based upon expected free energy) to
% select the next action. Previous actions are realised variables and are
% used when evaluating the posterior beliefs over current states.
%
% In brief, the agent encodes beliefs about hidden states in the past
% conditioned on realised outcomes and actions. The resulting conditional
% expectations determine the (path integral) of free energy that then
% determines an empirical prior over the next action, from which the next
% realised action sampled
%
% In addition to state estimation and policy selection, the scheme also
% updates model parameters; including the state transition matrices,
% mapping to outcomes and the initial state. This is useful for learning
% the context. Likelihood and prior probabilities can be specified in terms
% of concentration parameters (of a Dirichlet distribution (a,b,c,..). If
% the corresponding (A,B,C,..) are supplied, they will be used to generate
% outcomes.
%
% If supplied with a structure array, this routine will automatically step
% through the implicit sequence of epochs (implicit in the number of
% columns of the array). If the array has multiple rows, each row will be
% treated as a separate model or agent. This enables agents to communicate
% through acting upon a common set of hidden factors, or indeed sharing the
% same outcomes.
%
% See also: spm_MDP, which uses multiple future states and a mean field
% approximation for control states - but allows for different actions at
% all times (as in control problems).
%
% See also: spm_MDP_VB_X,  which is the corresponding variational message
% passing scheme for fixed policies; i.e., ordered sequences of actions
% that are specified a priori.
%__________________________________________________________________________
% Copyright (C) 2019 Wellcome Trust Centre for Neuroimaging

% Karl Friston
% $Id: spm_MDP_VB_XXX.m 8379 2023-01-02 16:14:35Z karl $


% deal with a sequence of trials
%==========================================================================

% options
%--------------------------------------------------------------------------
try, OPTIONS.plot;  catch, OPTIONS.plot  = 0; end
try, OPTIONS.D;     catch, OPTIONS.D     = 0; end

% check MDP specification
%--------------------------------------------------------------------------
MDP = spm_MDP_check(MDP);

% handle multiple trials, ensuring parameters (and posteriors) are updated
%==========================================================================
if size(MDP,2) > 1

    % plotting options
    %----------------------------------------------------------------------
    GRAPH        = OPTIONS.plot;
    OPTIONS.plot = 0;

    for i = 1:size(MDP,2)                  % number of MDPs
        for m = 1:size(MDP,1)              % number of trials
            if i > 1                       % if previous inversions

                % update concentration parameters
                %----------------------------------------------------------
                MDP(m,i)  = spm_MDP_update(MDP(m,i),OUT(m,i - 1));

                % update initial states (post-diction)
                %----------------------------------------------------------
                if any(OPTIONS.D)
                    nD = numel(MDP(m,i).D);
                    if numel(OPTIONS.D) ~= nD
                        OPTIONS.D = ones(nD,1);
                    end
                    for f = 1:nD
                        if OPTIONS.D(f)
                            MDP(m,i).D{f} = OUT(m,i - 1).X{f}(:,end);
                        end
                    end
                end
            end
        end

        % solve this trial (for all models synchronously)
        %------------------------------------------------------------------
        OUT(:,i) = spm_MDP_VB_XXX(MDP(:,i),OPTIONS);
        fprintf('trial %i\n',i);

        % Bayesian model reduction
        %------------------------------------------------------------------
        if isfield(OPTIONS,'BMR')
            if isfield(OPTIONS.BMR,'fun')
                bmrfun = OPTIONS.BMR.fun;
            else
                bmrfun = @(MDP,BMR) spm_MDP_VB_sleep(MDP,BMR);
            end
            for m = 1:size(MDP,1)
                OUT(m,i) = bmrfun(OUT(m,i),OPTIONS.BMR);
            end
        end

    end

    % plot summary statistics - over trials
    %----------------------------------------------------------------------
    MDP = OUT;
    if GRAPH
        if ishandle(GRAPH)
            figure(GRAPH); clf
        else
            spm_figure('GetWin','MDP'); clf
        end
        spm_MDP_VB_game(MDP(1,:))
    end
    return
end


% set up and preliminaries
%==========================================================================

% number of outcomes T and policies U
%--------------------------------------------------------------------------
[T,U] = spm_MDP_get_T(MDP);

% defaults
%--------------------------------------------------------------------------
try, alpha = MDP(1).alpha; catch, alpha = 512;  end % action precision
try, eta   = MDP(1).eta;   catch, eta   = 1;    end % learning rate
try, chi   = MDP(1).chi;   catch, chi   = 1/64; end % Occam window updates
try, N     = MDP(1).N;     catch, N     = 0;    end % depth of policy search
N          = min(N,T);

% initialise model-specific parameters
%==========================================================================
for m = 1:size(MDP,1)

    % number of outsomes, states, controls and policies
    %----------------------------------------------------------------------
    Ng(m) = numel(MDP(m).A);               % number of outcome factors
    Nf(m) = numel(MDP(m).B);               % number of hidden factors
    Np(m) = size(U{m},1);                  % number of policies
    for f = 1:Nf(m)
        Ns(m,f) = size(MDP(m).B{f},1);     % number of hidden states
        Nu(m,f) = size(MDP(m).B{f},3);     % number of hidden controls
    end
    for g = 1:Ng(m)
        No(m,g) = size(MDP(m).A{g},1);     % number of outcomes
    end

    % parameters of generative model and policies
    %======================================================================

    % likelihood model (for a partially observed MDP)
    %----------------------------------------------------------------------
    for g = 1:Ng(m)

        % ensure probabilities are normalised  : A
        %------------------------------------------------------------------
        if ~islogical(MDP(m).A{g})
            MDP(m).A{g} = full(spm_norm(MDP(m).A{g}));
        end

        % parameters (concentration parameters): a
        %------------------------------------------------------------------
        if isfield(MDP,'a')
            A{m,g}  = MDP(m).a{g};
        else
            A{m,g}  = MDP(m).A{g}*512;
        end

        % normalised likelihood
        %------------------------------------------------------------------
        L{m,g} = spm_norm(A{m,g});

        % prior concentration paramters
        %------------------------------------------------------------------
        if isfield(MDP,'a')
            pA{m,g} = MDP(m).a{g};
        end

        % prior concentration parameters and novelty (W)
        %------------------------------------------------------------------
        if isfield(MDP,'a')
            W{m,g}  = spm_wnorm(MDP(m).a{g}).*(MDP(m).a{g} > 0);
        else
            W{m,g}  = false;
        end

        % and ambiguity (H) (for computation of expected free energy: G)
        %------------------------------------------------------------------
        if islogical(A{m,g})
            H{m,g}  = false;
        else
            H{m,g}  = sum(L{m,g}.*spm_log(L{m,g}),1);
        end
    end


    % transition probabilities (priors)
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        for j = 1:Nu(m,f)

            % controlable transition probabilities : B
            %--------------------------------------------------------------
            MDP(m).B{f}(:,:,j) = spm_norm(MDP(m).B{f}(:,:,j));

            % parameters (concentration parameters): b
            %--------------------------------------------------------------
            if isfield(MDP,'b')
                fB{m,f}(:,:,j) = spm_norm(MDP(m).b{f}(:,:,j));
            else
                fB{m,f}(:,:,j) = spm_norm(MDP(m).B{f}(:,:,j));
            end
            lnB{m,f}           = spm_log(fB{m,f});

        end

        % prior concentration paramters for novelty
        %------------------------------------------------------------------
        if isfield(MDP,'b')
            pB{m,f} = MDP(m).b{f};
        end

        % priors over initial hidden states: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'d')
            D{m,f} = spm_norm(MDP(m).d{f});
        elseif isfield(MDP,'D')
            D{m,f} = spm_norm(MDP(m).D{f});
        else
            D{m,f} = spm_norm(ones(Ns(m,f),1));
        end

        % prior concentration paramters for novelty
        %------------------------------------------------------------------
        if isfield(MDP,'d')
            pD{m,f} = MDP(m).d{f};
        end

        % priors over policies: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'e')
            E{m,f} = spm_norm(MDP(m).e{f});
        elseif isfield(MDP,'E')
            E{m,f} = spm_norm(MDP(m).E{f});
        else
            E{m,f} = spm_norm(ones(Nu(m,f),1));
        end

        % prior concentration paramters for habits
        %------------------------------------------------------------------
        if isfield(MDP,'e')
            pE{m,f} = MDP(m).e{f};
        end

    end


    % prior preferences (log probabilities) : C (Z)
    %----------------------------------------------------------------------
    for g = 1:Ng(m)

        if isfield(MDP,'c')
            C{m,g}  = spm_psi(MDP(m).c{g}(:,1) + 1/32);
            pC{m,g} = MDP(m).c{g}(:,1);
        elseif isfield(MDP,'C')
            C{m,g}  = MDP(m).C{g}(:,1);
        else
            C{m,g}  = zeros(No(m,g),1);
        end

        % prior preferences (log probabilities) : C
        %------------------------------------------------------------------
        C{m,g} = spm_log(spm_softmax(C{m,g}));

    end


    % initialise posterior expectations (Q) of hidden states at the current
    % time (X) and over time (S)
    %======================================================================
    for f = 1:Nf(m)
        S{m,f} = zeros(Ns(m,f),T,T) + 1/Ns(m,f);
        X{m,f} = repmat(D{m,f},1,T);
        for t = 1:T
            Q{m,f,t} = D{m,f};
        end
    end

    % initialise posteriors over control states
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        for t = 1:T
            P{m,f,t} = E{m,f};
        end
    end

    % if states have not been specified, set to 0
    %----------------------------------------------------------------------
    d  = zeros(Nf(m),T);
    try
        i    = find(MDP(m).s);
        d(i) = MDP(m).s(i);
    end
    MDP(m).s = d;

    % if controls have not been specified, set to 0
    %----------------------------------------------------------------------
    d  = zeros(Nf(m),T);
    try
        i    = find(MDP(m).u);
        d(i) = MDP(m).u(i);
    end
    MDP(m).u = d;

    % if outcomes have not been specified set to 0
    %----------------------------------------------------------------------
    d  = zeros(Ng(m),T);
    try
        i    = find(MDP(m).o);
        d(i) = MDP(m).o(i);
    end
    MDP(m).o = d;

end

% ensure any outcome generating agent is updated first
%--------------------------------------------------------------------------
[M,MDP] = spm_MDP_get_M(MDP,T,Ng);


% belief updating over successive time points
%==========================================================================
for t = 1:T

    % generate hidden states for each agent or model
    %======================================================================
    for m = M(t,:)

        % sample state, if not specified
        %------------------------------------------------------------------
        for f = 1:Nf(m)

            % propagate control state
            %--------------------------------------------------------------
            if ~MDP(m).u(f,t) && ~any(U{m}(:,f))

                % unless it is the initial control state
                %----------------------------------------------------------
                if t > 1
                    MDP(m).u(f,t) = MDP(m).u(f,t - 1);
                else
                    pu            = spm_norm(MDP(m).E{f});
                    MDP(m).u(f,t) = find(rand < cumsum(pu),1);
                end
            end

            % the next state is generated by state transititions
            %--------------------------------------------------------------
            if ~MDP(m).s(f,t)

                % unless it is the initial state
                %----------------------------------------------------------
                if t > 1
                    ps = MDP(m).B{f}(:,MDP(m).s(f,t - 1),MDP(m).u(f,t - 1));
                else
                    ps = spm_norm(MDP(m).D{f});
                end

                MDP(m).s(f,t) = find(rand < cumsum(ps),1);
            end
        end
    end

    % share states if specified in MDP.m
    %----------------------------------------------------------------------
    for m = M(t,:)
        for f = 1:Nf(m)
            if isfield(MDP(m),'m')
                n = MDP(m).m(f);
                if n
                    MDP(m).s(f,t) = MDP(n).s(f,t);
                end
            end
        end
    end

    % generate outcomes for each agent or model
    %======================================================================
    for m = M(t,:)

        % posterior predictive density over hidden states
        %------------------------------------------------------------------
        for f = 1:Nf(m)

            % under control
            %--------------------------------------------------------------
            if t > 1
                xq{m,f} = spm_dot(fB{m,f},P(m,f,t - 1))*Q{m,f,t - 1};
            else
                xq{m,f} = Q{m,f,t};
            end

        end

        % sample outcome, if not specified
        %------------------------------------------------------------------
        for g = 1:Ng(m)

            % if outcome is not specified
            %--------------------------------------------------------------
            if ~MDP(m).o(g,t)

                % outcome is generated by model n
                %----------------------------------------------------------
                if MDP(m).n(g,t) > 0

                    n    = MDP(m).n(g,t);
                    if n == m

                        % outcome that minimises free energy (i.e.,
                        % maximises accuracy)
                        %----------------------------------------------
                        F             = spm_dot(spm_psi(A{m,g}),xq(m,:));
                        po            = spm_softmax(F*512);
                        MDP(m).o(g,t) = find(rand < cumsum(po),1);

                    else

                        % outcome from model n
                        %--------------------------------------------------
                        MDP(m).o(g,t) = MDP(n).o(g,t);

                    end

                elseif MDP(m).n(g,t) < 0

                    % outcome is generated by all models or agents
                    %------------------------------------------------------
                    Fm{g,m} = spm_dot(spm_psi(A{m,g}),xq(m,:));


                else

                    % or sample from likelihood, given hidden state
                    %------------------------------------------------------
                    ind           = num2cell(MDP(m).s(:,t));
                    po            = MDP(m).A{g}(:,ind{:});
                    MDP(m).o(g,t) = find(rand < cumsum(po),1);

                end
            end
        end
    end



    % get probabilistic outcomes (O{m,g,t}) from samples or subordinate level
    %======================================================================
    for m = M(t,:)
        for g = 1:Ng(m)

            % share outcomes if specified in MDP.m
            %--------------------------------------------------------------
            if MDP(m).n(g,t) < 0
                F             = sum([Fm{g,:}],2);
                O{m,g,t}      = spm_softmax(F);
                po            = spm_softmax(F*512);
                MDP(m).o(g,t) = find(rand < cumsum(po),1);
            else

                % otherwise use above outcomes
                %----------------------------------------------------------
                O{m,g,t}      = full(sparse(MDP(m).o(g,t),1,1,No(m,g),1));
            end
        end
    end



    % or generate outcomes from a subordinate MDP
    %======================================================================
    for m = M(t,:)

        if isfield(MDP,'link')

            % use previous inversions (if available) to reproduce outcomes
            %--------------------------------------------------------------
            try
                mdp = MDP(m).mdp(t);
            catch
                try
                    mdp     = spm_MDP_update(MDP(m).MDP(t),MDP(m).mdp(t - 1));
                catch
                    try
                        mdp = spm_MDP_update(MDP(m).MDP(1),MDP(m).mdp(t - 1));
                    catch
                        mdp = MDP(m).MDP(1);
                    end
                end
            end

            % priors over states (of subordinate level)
            %--------------------------------------------------------------
            mdp.factor = [];
            for f = 1:size(MDP(m).link,1)
                for g = 1:size(MDP(m).link,2)
                    if ~isempty(MDP(m).link{f,g})

                        % subordinate state has hierarchical constraints
                        %--------------------------------------------------
                        mdp.factor(end + 1) = f;

                        % empirical priors over initial states
                        %--------------------------------------------------
                        O{m,g,t} = spm_dot(L{m,g},xq(m,:));
                        mdp.D{f}  = MDP(m).link{f,g}*O{m,g,t};

                        % outcomes (i.e., states) are generated by model n
                        %--------------------------------------------------
                        if MDP(m).n(g,t)
                            n    = MDP(m).n(g,t);
                            if m == n
                                ps         = MDP(m).link{f,g}(:,MDP(m).o(g,t));
                                mdp.s(f,1) = find(ps);
                            else
                                mdp.s(f,1) = MDP(n).mdp(t).s(f,1);
                            end
                        end

                        % hidden state for lower level is the outcome
                        %--------------------------------------------------
                        try
                            mdp.s(f,1) = mdp.s(f,1);
                        catch
                            ps         = MDP(m).link{f,g}(:,MDP(m).o(g,t));
                            mdp.s(f,1) = find(ps);
                        end

                    end
                end
            end


            % empirical prior preferences
            %--------------------------------------------------------------
            if isfield(MDP,'linkC')
                for f = 1:size(MDP(m).linkC,1)
                    for g = 1:size(MDP(m).linkC,2)
                        if ~isempty(MDP(m).linkC{f,g})
                            O{m,g,t} = spm_dot(L{m,g},xq(m,:));
                            mdp.C{f} = spm_log(MDP(m).linkC{f,g}*O{m,g,t});
                        end
                    end
                end
            end

            % empirical priors over policies
            %--------------------------------------------------------------
            if isfield(MDP,'linkE')
                mdp.factorE = [];
                for g = 1:size(MDP(m).linkE,2)
                    if ~isempty(MDP(m).linkE{g})
                        O{m,g,t} = spm_dot(L{m,g},xq(m,:));
                        mdp.E    = MDP(m).linkE{g}*O{m,g,t};
                    end
                end
            end


            % infer hidden states at lower level (outcomes at this level)
            %==============================================================
            MDP(m).mdp(t) = spm_MDP_VB_XXX(mdp);


            % get inferred outcomes from subordinate MDP
            %==============================================================
            for f = 1:size(MDP(m).link,1)
                for g = 1:size(MDP(m).link,2)
                    if ~isempty(MDP(m).link{f,g})
                        O{m,g,t} = MDP(m).link{f,g}'*MDP(m).mdp(t).X{f}(:,1);
                    end
                end
            end

            % if hierarchical preferences, these contribute to outcomes ...
            %--------------------------------------------------------------
            if isfield(MDP,'linkC')
                for f = 1:size(MDP(m).linkC,1)
                    for g = 1:size(MDP(m).linkC,2)
                        if ~isempty(MDP(m).linkC{f,g})
                            indC     = sparse(MDP(m).mdp(t).o(f,:)',1:length(MDP(m).mdp(t).o(f,:)),ones(length(MDP(m).mdp(t).o(f,:)),1),size(MDP(m).mdp(t).C{f},1),size(MDP(m).mdp(t).C{f},2));
                            O{m,g,t} = spm_softmax(spm_log(O{m,g,t}) + MDP(m).linkC{f,g}'*sum((indC.*(MDP(m).mdp(t).C{f})),2));
                        end
                    end
                end
            end

            % ... and the same for policies
            %--------------------------------------------------------------
            if isfield(MDP,'linkE')
                for g = 1:size(MDP(m).linkE,2)
                    if ~isempty(MDP(m).linkE{g})
                        O{m,g,t} = spm_softmax(spm_log(O{m,g,t}) + spm_log(MDP(m).linkE{g}'*MDP(m).mdp(t).R(:,end)));
                    end
                end
            end

            % ensure DEM starts with final states from previous inversion
            %--------------------------------------------------------------
            if isfield(MDP(m).MDP,'demi')
                MDP(m).MDP.DEM.G(1).x = MDP(m).mdp(t).dem(end).pU.x{1}(:,end);
                MDP(m).MDP.DEM.M(1).x = MDP(m).mdp(t).dem(end).qU.x{1}(:,end);
            end

        end % end of hierarchical mode (link)


        % or generate outcome likelihoods from a variational filter
        %==================================================================
        if isfield(MDP,'demi')

            % use previous inversions (if available)
            %--------------------------------------------------------------
            try
                MDP(m).dem(t) = spm_ADEM_update(MDP(m).dem(t - 1));
            catch
                MDP(m).dem(t) = MDP(m).DEM;
            end

            % get prior over outcomes
            %--------------------------------------------------------------
            for g = 1:Ng(m)
                O{m,g,t} = spm_dot(L{m,g},xq(m,:));
            end

            % get posterior outcome from Bayesian filtering
            %--------------------------------------------------------------
            MDP(m).dem(t) = spm_MDP_DEM(MDP(m).dem(t),...
                MDP(m).demi,O(m,:,t),MDP(m).o(:,t));

            for g = 1:Ng(m)
                O{m,g,t} = MDP(m).dem(t).X{g}(:,end);
            end

        end % end outcomes from Bayesian filter

    end % end loop over models or agents


    % Bayesian belief updating hidden states (Q) and controls (P)
    %======================================================================
    for m = M(t,:)

        % belief propagation (B) under policy k
        %------------------------------------------------------------------
        for f = 1:Nf(m)
            for k = 1:Np(m)
                if U{m}(k,f)
                    B{m,f,k} = fB{m,f}(:,:,U{m}(k,f));
                else
                    B{m,f,k} = spm_dot(fB{m,f},P(m,f,t));
                end
            end
        end

        % empirical prior over hidden states (and actions) at this time
        %------------------------------------------------------------------
        for f = 1:Nf(m)
            if t > 1

                % use transition probabilities
                %----------------------------------------------------------
                Q{m,f,t} = spm_dot(fB{m,f},P(m,f,t - 1))*Q{m,f,t - 1};

            else

                % use empirical priors over initial states
                %----------------------------------------------------------
                Q{m,f,t} = D{m,f};

            end
        end


        % posterior over hidden states (Q) and expected free energy (G)
        %==================================================================
        [G,Q,F] = spm_forwards(O,Q,L,B,C,H,W,t,T,min(T,t + N),m);


        % save marginal posteriors over hidden states: c.f., working memory
        %------------------------------------------------------------------
        for f = 1:Nf(m)
            X{m,f}(:,t)       = Q{m,f,t};
            for i = 1:T
                S{m,f}(:,i,t) = Q{m,f,i};
            end
        end

        % posterior predictive density Y{g,t}
        %------------------------------------------------------------------
        for g = 1:Ng(m)
            MDP(m).Y{g,t} = spm_dot(L{m,g},Q(m,:,t));
        end

        % posterior beliefs about policies (R) and precision (w)
        %------------------------------------------------------------------
        R{m}(:,t)   = spm_softmax(G);
        w{m}(t)     = R{m}(:,t)'*spm_log(R{m}(:,t));
        v{m}(t)     = R{m}(:,t)'*G;

        % end policy search
        %==================================================================

        % check for residual uncertainty (in hierarchical schemes)
        %------------------------------------------------------------------
        if isfield(MDP,'factor')

            for f = MDP(m).factor(:)'
                qx      = X{m,f}(:,1);
                SF(m,f) = qx'*spm_log(qx);
            end

            % break if there is no further uncertainty to resolve
            %--------------------------------------------------------------
            if sum(SF(:)) > - chi
                T = t;
            end
        end

        % posterior over hidden control states (P)
        %==================================================================
        Z     = 0;
        for f = 1:Nf(m)
            if t > 1

                % approximate posterior
                %----------------------------------------------------------
                Lu       = spm_dot(spm_dot(lnB{m,f},Q{m,f,t}),Q{m,f,t - 1});
                Lu       = squeeze(Lu) + spm_log(P{m,f,t - 1});
                P{m,f,t} = spm_softmax(Lu);

                % ELBO or free energy of control states
                %----------------------------------------------------------
                Z        = Z + P{m,f,t}'*(spm_log(P{m,f,t}) - Lu);

            end
        end

        % policy selection
        %==================================================================
        if t < T

            % record emprical prior over policies (R) and sample a policy (r)
            %--------------------------------------------------------------
            Ru            = spm_softmax(alpha*log(R{m}(:,t)));
            r(m,t)        = find(rand < cumsum(Ru),1);
            MDP(m).r(:,t) = r(m,t);

            % control state
            %--------------------------------------------------------------
            for f = 1:Nf(m)

                % control state if unspecified
                %----------------------------------------------------------
                if ~MDP(m).u(f,t)
                    MDP(m).u(f,t) = U{m}(r(m,t),f);
                end

                % collapse beliefs about controlled state
                %------------------------------------------------------
                if U{m}(r(m,t),f)
                    P{m,f,t} = sparse(U{m}(r(m,t),f),1,1,Nu(m,f),1);
                end

            end

        end % end of state and action selection


        % active (likelihood) learning
        %==================================================================

        % mapping from hidden states to outcomes: a
        %------------------------------------------------------------------
        LEARN     = 0;
        if isfield(MDP(m),'a') && LEARN
            for g = 1:Ng(m)

                da     = spm_cross(O(m,g,t),Q{m,:,t});
                da     = reshape(da,[No(m,g),Ns(m,:)]);
                da     = da.*(A{m,g} > 0);

                % update likelihood Dirichlet parameters
                %----------------------------------------------------------
                A{m,g} = A{m,g} + da*eta;
                L{m,g} = spm_norm(A{m,g});

                % prior concentration parameters and novelty (W)
                %----------------------------------------------------------
                W{m,g} = spm_wnorm(A{m,g}).*(A{m,g} > 0);

                % and ambiguity (H)
                %----------------------------------------------------------
                H{m,g} = full(sum(L{m,g}.*spm_log(L{m,g}),1));

            end
        end

        % (ELBO) free energy: states, policies and controls
        %------------------------------------------------------------------
        MDP(m).F(t) = F;
        MDP(m).G{t} = G;
        MDP(m).Z(t) = Z;


    end % end of loop over models (agents)

    % terminate evidence accumulation
    %----------------------------------------------------------------------
    if t == T
        for m = 1:size(MDP,1)
            MDP(m).o  = MDP(m).o(:,1:T);        % outcomes at 1,...,T
            MDP(m).s  = MDP(m).s(:,1:T);        % states   at 1,...,T
            MDP(m).u  = MDP(m).u(:,1:T - 1);    % control  at 1,...,T - 1
        end
        break;
    end

end % end of loop over time

% initialise simulated neuronal respsonses
%--------------------------------------------------------------------------
n     = 16;
for m = 1:size(MDP,1)
    for f = 1:Nf(m)
        xn{m,f} = zeros(n,Ns(m,f),T,T);
    end
end

% loop over models to accumulate Dirichlet parameters and prepare outputs
%==========================================================================
for m = 1:size(MDP,1)

    % Backwards pass for posteriors over hidden states
    %----------------------------------------------------------------------
%     if any(isfield(MDP,{'b','d'}))
%         for t = 1:T
%             Xt     = spm_backwards(O,Q,L,B,r,t,T,m);
%             Xt     = reshape(Xt,[Ns(m,:),1]);
%             for f = 1:Nf(m)
%                 X{m,f}(:,t) = spm_margin(Xt,f);
%             end
%         end
%     end

    % learning - accumulate concentration parameters
    %======================================================================
    for t = 1:T


        % likelihood mapping from hidden states to outcomes: a
        %------------------------------------------------------------------
        if isfield(MDP(m),'a')
            for g = 1:Ng(m)
                da = spm_cross(O(m,g,t),Q{m,:,t});
                da = reshape(da,[No(m,g),Ns(m,:)]);
                da = da.*(A{m,g} > 0);
                MDP(m).a{g} = MDP(m).a{g} + da*eta;
            end
        end

        % mapping from hidden states to hidden states: b(u)
        %------------------------------------------------------------------
        if isfield(MDP,'b') && t < T
            for f = 1:Nf(m)
                for j = 1:Nu(m,f)
                    db  = spm_cross(X{m,f}(:,t + 1),X{m,f}(:,t)');

                    % Control state only changes when actionable
                    %------------------------------------------------------
                    if any(U{m}(:,f))
                        db = db*P{m,f,t}(j);
                    else
                        db = db*P{m,f,T}(j);
                    end

                    db  = db.*(MDP(m).b{f}(:,:,j) > 0);
                    MDP(m).b{f}(:,:,j) = MDP(m).b{f}(:,:,j) + db*eta;
                end
            end
        end
    end

    % accumulation of prior preferences: (c)
    %----------------------------------------------------------------------
    if isfield(MDP,'c') && t < T
        for g = 1:Ng(m)
            dc = O{m,g,t + 1};
            dc = dc.*(MDP(m).c{g} > 0);
            MDP(m).c{g} = MDP(m).c{g} + dc*eta;
        end
    end

    % initial hidden states:
    %----------------------------------------------------------------------
    if isfield(MDP,'d')
        for f = 1:Nf(m)
            dd = X{m,f}(i,1);
            dd = dd.*(MDP(m).d{f} > 0);
            MDP(m).d{f} = MDP(m).d{f} + dd*eta;
        end
    end

    % initial hidden states:
    %----------------------------------------------------------------------
    if isfield(MDP,'e')
        for f = 1:Nf(m)
            de = P{m,f,end};
            de = de.*(MDP(m).e{f} > 0);
            MDP(m).e{f} = MDP(m).e{f} + de*eta;
        end
    end

    % policies
    %----------------------------------------------------------------------
    if isfield(MDP,'e')
        de   = 0;
        for t = 1:(T - 1)
            de = de + R{m}(:,t);
        end
        MDP(m).e = MDP(m).e + de*eta;
    end

    % (negative) free energy of parameters (complexity): outcome specific
    %======================================================================
    for g = 1:Ng(m)
        if isfield(MDP,'a')
            MDP(m).Fa(g) = - spm_KL_dir(MDP(m).a{g},pA{m,g});
        end
        if isfield(MDP,'c')
            MDP(m).Fc(f) = - spm_KL_dir(MDP(m).c{g},pC{g});
        end
    end

    % (negative) free energy of parameters: state specific
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        if isfield(MDP,'b')
            MDP(m).Fb(f) = - spm_KL_dir(MDP(m).b{f},pB{m,f});
        end
        if isfield(MDP,'d')
            MDP(m).Fd(f) = - spm_KL_dir(MDP(m).d{f},pD{m,f});
        end
        if isfield(MDP,'e')
            MDP(m).Fe(f) = - spm_KL_dir(MDP(m).e{f},pE{m,f});
        end
    end




    % simulated electrophysiological responses
    %======================================================================

    % simulated dopamine (or cholinergic) responses: assuming a
    % monoexponential kernel
    %----------------------------------------------------------------------
    h     = exp(-(0:(n - 1))/2);
    h     = h/sum(h);
    wn{m} = kron(w{m},ones(1,n));
    wn{m} = conv(wn{m},[spm_zeros(h) h],'same');
    dn{m} = gradient(wn{m}(:));


    % Belief updating about hidden states: assuming a kernel or impulse
    % response function with a cumulative gamma distribution
    %----------------------------------------------------------------------
    h     = spm_Gcdf(0:(n - 1),n/4,1);
    for f = 1:Nf(m)
        for i = 1:Ns(m,f)
            for j = 1:T
                for k = 1:T
                    if k == 1
                        h0 = 1/Ns(m,f);
                    else
                        h0 = S{m,f}(i,j,k - 1);
                    end
                    ht     = S{m,f}(i,j,k);
                    xn{m,f}(:,i,j,k) = h*(ht - h0) + h0;
                end
            end
        end
    end

    % sum to one contraint
    %----------------------------------------------------------------------
    for i = 1:n
        for j = 1:T
            for k = 1:T
                xn{m,f}(i,:,j,k) = xn{m,f}(i,:,j,k)/sum(xn{m,f}(i,:,j,k));
            end
        end
    end

    % belief updating about policies
    %----------------------------------------------------------------------
    u0    = spm_softmax(ones(Np(m),1));
    for k = 1:Np(m)
        for t = 1:(T - 1)
            if t == 1
                h0 = u0(k);
            else
                h0 = R{m}(k,t - 1);
            end
            ht         = R{m}(k,t);
            j          = (1:n) + (t - 1)*n;
            un{m}(k,j) = (h*(ht - h0) + h0);
        end
    end


    % assemble results and place in NDP structure
    %======================================================================
    MDP(m).T  = T;            % number of outcomes
    MDP(m).U  = U{m};         % policies
    MDP(m).R  = R{m};         % conditional expectations over policies
    MDP(m).C  = C(m,:);       % utility
    MDP(m).X  = X(m,:);       % conditional expectations over states
    MDP(m).P  = P(m,:,:);     % conditional expectations over controls
    MDP(m).O  = O(m,:,:);     % outcomes

    MDP(m).O  = squeeze(MDP(m).O);
    MDP(m).P  = squeeze(MDP(m).P);

    MDP(m).v  = v{m};         % expected free energy  over policies
    MDP(m).w  = w{m};         % precision of beliefs about policies
    MDP(m).xn = xn(m,:);      % simulated neuronal encoding of states
    MDP(m).un = un{m};        % simulated neuronal encoding of policies
    MDP(m).wn = wn{m};        % simulated neuronal encoding of precision
    MDP(m).dn = dn{m};        % simulated dopamine responses (phasic)


end % end loop over models (m)


% plot
%==========================================================================
if OPTIONS.plot
    if ishandle(OPTIONS.plot)
        figure(OPTIONS.plot); clf
    else
        spm_figure('GetWin','MDP'); clf
    end
    spm_MDP_VB_trial(MDP(1))
end


% auxillary functions
%==========================================================================
function [G,P,F] = spm_forwards(O,P,A,B,C,H,W,t,T,N,m)
% deep tree search over policies or paths
%--------------------------------------------------------------------------
% FORMAT [G,Q,F] = spm_forwards(O,P,A,B,C,H,W,t,T,N,m)
% O{m,g,t} - cell array of outcome probabilities for modality g
% P{m,f,t}   cell array of priors over states
% A{m,g}   - likelihood mappings from hidden states
% B{m,f,k} - belief propagators (action dependent probability transitions)
% C{m,g}   - cost: log priors over future outcomes
% H{m,g}   - state dependent ambiguity
% W{m,g}   - state and outcome dependent novelty
% t        - current time point
% T        - time horizon
% N        - policy horizon
% m        - model or agent to update
%
% G(k)     - expected free energy over k policies
% Q{m,f,t} - posterior over states
% F        - variational free energy (negative or ELBO)
%
%  This subroutine performs a deep tree search over sequences of actions to
%  evaluate the expected free energy over policies or paths. Crucially, it
%  only searches likely policies under likely hidden states in the future.
%  This search is sophisticated; in the sense that posterior beliefs are
%  updated on the basis of future outcomes to evaluate the free energy
%  under each outcome. The resulting  average is then accumulated to
%  furnish a path integral of expected free energy for the next action.
%  This routine operates recursively by updating predictive posteriors over
%  hidden states and their most likely outcomes.
%__________________________________________________________________________


% Posterior over hidden states based on likelihood (L) and priors (P)
%==========================================================================
G        = zeros(size(B,3),1);                    % log priors over actions

% variational (Bayesian) belief updating and free energy (ELBO)
%--------------------------------------------------------------------------
[Q,F]    = spm_VBX(O(m,:,t),P(m,:,t),A(m,:),'exact');
Q        = Q(1:size(P,2));
P(m,:,t) = Q;


% terminate search at time horizon
%--------------------------------------------------------------------------
if t == T || numel(G) == 1, return, end

%% Expected free energy of subsequent action
%==========================================================================
for k = 1:size(B,3)                               % search over actions

    % (negative) expected free energy
    %----------------------------------------------------------------------
    for f = 1:size(B,2)
        Q{f,k} = B{m,f,k}*P{m,f,t};               % predictive posterior
    end

    for g  = 1:size(A,2)                          % for all modalities

        % predictive posterior and prior over outcomes
        %------------------------------------------------------------------
        qo   = spm_dot(A{m,g},Q(:,k));            % predictive outcomes
        po   = C{m,g};                            % predictive log prior

        % G(k): risk
        %-----------------------------------------------------------------
        G(k) = G(k) - qo'*(spm_log(qo) - po);

        % G(k): ambiguity
        %------------------------------------------------------------------
        if any(H{m,g},'all')
            G(k) = G(k) + spm_dot(H{m,g},Q(:,k));
        end

        % expected information gain about parameters (novelty)
        %------------------------------------------------------------------
        if any(W{m,g},'all')
            G(k) = G(k) + qo'*spm_dot(W{m,g},Q(:,k));
        end

    end
end

%% deep (recursive) search over action sequences ( i.e., paths)
%==========================================================================
if t < N

    % probability over action (terminating search at a suitable threshold)
    %----------------------------------------------------------------------
    u     = spm_softmax(G);
    k     = u <= max(u)/16;
    u(k)  = 0;
    G(k)  = - 64;
    for k = 1:size(B,3)                      % search over actions
        q = spm_vec(spm_cross(Q(:,k)));
        if u(k)                              % evaluating plausible paths

            %  evaluate expected free energy for plausible hidden states
            %--------------------------------------------------------------
            j     = find(q > max(q)/16);
            if length(j) > 16
                [q,j] = sort(q,'descend');
                j     = j(1:16);
            end
            for i = j(:)'

                % outcome probabilities under hidden state (i)
                %----------------------------------------------------------
                for g = 1:size(A,2)
                    O{m,g,t + 1} = A{m,g}(:,i);
                end

                % prior over subsequent action under this hidden state
                %----------------------------------------------------------
                P(m,:,t + 1) = Q(:,k);
                E     = spm_forwards(O,P,A,B,C,H,W,t + 1,T,N,m);

                % expected free energy marginalised over subsequent action
                %----------------------------------------------------------
                K(i)  = spm_softmax(E)'*E;

            end

            % accumulate expected free energy marginalised over states
            %--------------------------------------------------------------
            G(k) = G(k) + K(j)*q(j);

        end % plausible paths
    end % search over actions


    % Predictive posterior over hidden states
    %----------------------------------------------------------------------
    u     = spm_softmax(G);
    for k = 1:size(B,2)
        for f = 1:size(B,2)
            R{f}  = u(k)*Q{f,k};
        end
    end
    P(m,:,t + 1) = R;

end


function [L] = spm_backwards(O,Q,A,B,u,t,T,m)
% Backwards smoothing to evaluate posterior over initial states
%--------------------------------------------------------------------------
% O{g}   - cell array of outcome probabilities for modality g
% Q{t}   - posterior expectations over vectorised hidden states
% A{g}   - likelihood mappings from hidden states
% B{k}   - belief propagators (action dependent probability transitions)
% u{t}   - posterior expectations over actions
% t      - current time point
% T      - time horizon
% m      - agent or model
%
% L      - posterior over states at time t
%
%  This subroutine evaluates the posterior over initial states using a
%  backwards algorithm; namely, by evaluating the likelihood of each
%  initial state, given subsequent outcomes, under posterior expectations
%  about state transitions.


% initialise to posterior and accumulate likelihoods for each initial state
%--------------------------------------------------------------------------
L     = spm_vec(spm_cross(Q(m,:,t)));
for f = 1:size(B,2)
    b{f} = 1;
end
for j = (t + 1):T

    % belief propagation over hidden states
    %----------------------------------------------------------------------
    for f = 1:size(B,2)
        b{f} = B{m,f,u(j - 1)}*b{f};
    end
    p = 1;
    for f = 1:size(B,2)
        p = spm_kron(b{f},p);
    end

    % and accumulate likelihood
    %----------------------------------------------------------------------
    for i = 1:numel(L)
        for g = 1:numel(A)
            L(i) = L(i).*(O{m,g,j}'*A{g}(:,:)*p(:,i));
        end
    end
end

% marginal distribution over states
%--------------------------------------------------------------------------
L     = spm_norm(L(:));



function A  = spm_log(A)
% log of numeric array plus a small constant
%--------------------------------------------------------------------------
A           = log(A);
A(isinf(A)) = -32;


function A  = spm_norm(A)
% normalisation of a probability transition matrix (columns)
%--------------------------------------------------------------------------
A           = rdivide(A,sum(A,1));
A(isnan(A)) = 1/size(A,1);


function A  = spm_wnorm(A)
% summation of a probability transition matrix (columns)
% A = minus(log(A0),log(A)) + minus(1./A,1./A0) + minus(psi(A),psi(A0))
%   = minus(1./A,1./A0)/2 + ...
%--------------------------------------------------------------------------
A   = A + exp(-32);
A0  = sum(A,1);
A   = minus(log(A0),log(A)) + minus(1./A,1./A0) + minus(psi(A),psi(A0));
A   = 128*full(A);

function A  = spm_margin(A,i)
% marginalise a joint distribution
%--------------------------------------------------------------------------
if isvector(A), return, end
d     = 1:ndims(A);
d(i)  = [];
A     = sum(A,d(1));
for i = 2:numel(d)
    A = sum(A,d(i));
end
A     = squeeze(A);


return


function [T,U] = spm_MDP_get_T(MDP)
% FORMAT [T,U] = spm_MDP_get_T(MDP)
% returns number of policies, policy cell array and HMM flag
% MDP(m) - structure array of m MPDs
% T      - number of trials or updates
% U(m)   - indices of actions for m-th MDP
%
% This subroutine returns the policy matrix as a cell array (for each
% model) and the maximum number of updates.
%__________________________________________________________________________

for m = 1:size(MDP,1)

    if isfield(MDP(m),'U')
        if size(MDP(m).U,1) == 1

            % get number of factors and actions
            %--------------------------------------------------------------
            Nf    = numel(MDP(m).B);
            Nu    = ones(1,Nf);
            for f = 1:Nf
                if MDP(m).U(f)
                    Nu(f) = size(MDP(m).B{f},3);
                end
            end

            % get all combinations of actions
            %--------------------------------------------------------------
            U     = zeros(prod(Nu),Nf);
            for f = 1:Nf
                for j = 1:Nf
                    if j == f
                        k{j} = 1:Nu(j);
                    else
                        k{j} = ones(1,Nu(j));
                    end
                end
                u     = 1;
                for i = 1:Nf
                    u = kron(k{i},u);
                end

                % if controllable
                %----------------------------------------------------------
                if MDP(m).U(f)
                    U(:,f) = u(:);
                end
            end

            % policies (paths) for controllable factors
            %----------------------------------------------------------
            MDP(m).U = U;
            clear U

        end
    end

    if isfield(MDP(m),'U')

        % called with repeatable actions (U,T)
        %------------------------------------------------------------------
        T(m) = MDP(m).T;                    % number of updates
        U{m} = MDP(m).U;                    % allowable actions (Np,Nf)

    elseif isfield(MDP(m),'V')

        % full sequential policies (V)
        %------------------------------------------------------------------
        T(m) = MDP(m).T;                    % number of updates
        U{m} = MDP(m).V(1,:,:);             % allowable actions (1,Np,Nf)

    end

end

% number of time steps
%--------------------------------------------------------------------------
T = max(T);

return


function [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
% FORMAT [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
% returns an update matrix for multiple models
% MDP(m) - structure array of m MPDs
% T      - number of trials or updates
% Ng(m)  - number of output modalities for m-th MDP
%
% M      - update matrix for multiple models
% MDP(m) - structure array of m MPDs
%
% In some applications, the outcomes are generated by a particular model
% (to maximise free energy, based upon the posterior predictive density).
% The generating model is specified in the matrix MDP(m).n, with a row for
% each outcome modality, such that each row lists the index of the model
% responsible for generating outcomes.
%__________________________________________________________________________


for m = 1:size(MDP,1)

    % check size of outcome generating agent, as specified by MDP(m).n
    %----------------------------------------------------------------------
    if ~isfield(MDP(m),'n')
        MDP(m).n = zeros(Ng(m),T);
    elseif isempty(MDP(m).n)
        MDP(m).n = zeros(Ng(m),T);
    end
    if size(MDP(m).n,1) < Ng(m)
        MDP(m).n = repmat(MDP(m).n(1,:),Ng(m),1);
    end
    if size(MDP(m).n,2) < T
        MDP(m).n = repmat(MDP(m).n(:,1),1,T);
    end

    % mode of generating model (most frequent over outcome modalities)
    %----------------------------------------------------------------------
    n(m,:) = mode(MDP(m).n.*(MDP(m).n > 0),1);

end

% reorder list of model indices for each update
%--------------------------------------------------------------------------
n     = mode(n,1);
for t = 1:T
    if n(t) > 0
        M(t,:) = circshift((1:size(MDP,1)),[0 (1 - n(t))]);
    else
        M(t,:) = 1:size(MDP,1);
    end
end


return

function MDP = spm_MDP_update(MDP,OUT)
% FORMAT MDP = spm_MDP_update(MDP,OUT)
% moves Dirichlet parameters from OUT to MDP
% MDP - structure array (new)
% OUT - structure array (old)
%__________________________________________________________________________

% check for concentration parameters at this level
%--------------------------------------------------------------------------
try,  MDP.a = OUT.a; end
try,  MDP.b = OUT.b; end
try,  MDP.c = OUT.c; end
try,  MDP.d = OUT.d; end
try,  MDP.e = OUT.e; end

% check for concentration parameters at nested levels
%--------------------------------------------------------------------------
try,  MDP.MDP(1).a = OUT.mdp(end).a; end
try,  MDP.MDP(1).b = OUT.mdp(end).b; end
try,  MDP.MDP(1).c = OUT.mdp(end).c; end
try,  MDP.MDP(1).d = OUT.mdp(end).d; end
try,  MDP.MDP(1).e = OUT.mdp(end).e; end

return






%% NOTES: variational approximations to mapping from outcome space to
% approximate (variational) posterior using spm_VBX

Nf = [32,4,4];                            % number of levels per factor
Ng = [1024,2,8];                          % number of  outcomes per modality
T  = 128;                                 % temperature

% likelihood mapping
%--------------------------------------------------------------------------
for g = 1:numel(Ng)
    A{g} = rand([Ng(g),Nf]).^T;
    A{g} = bsxfun(@rdivide,A{g},sum(A{g},1));
end

% prior
%--------------------------------------------------------------------------
for f = 1:numel(Nf)
    P{f} = rand(Nf(f),1);
    P{f} = bsxfun(@rdivide,P{f},sum(P{f},1));
end

% outcomes
%--------------------------------------------------------------------------
for g = 1:numel(Ng)
    O{g} = rand(Ng(g),1);
    O{g} = bsxfun(@rdivide,O{g},sum(O{g},1));
end

% posterior and variational free energy from spm_VBX
%--------------------------------------------------------------------------
clf
[Q,F] = spm_VBX(O,P,A,'full'); disp(F)

subplot(2,2,1), bar(Q{1}), axis square, hold on
title({'first latent factor','iterative'})
subplot(2,2,2), bar(Q{2}), axis square, hold on
title({'second latent factor','iterative'})

[Q,F] = spm_VBX(O,P,A,'exact'); disp(F)

subplot(2,2,3), bar(Q{1}), axis square, hold off
title({'first latent factor','non-iterative'})
subplot(2,2,4), bar(Q{2}), axis square, hold off
title({'second latent factor','non-iterative'})

%% NOTES: on Dirichlet distributions: illustrating the impact of uncertainty
% about parameters
%--------------------------------------------------------------------------
a = (rand(8,1).^4)/64;

subplot(2,2,1)
plot(spm_dir_norm(a)), hold on, plot(spm_softmax(spm_psi(a))), hold off
title ('low Dirichlet counts')

a = a*256;

subplot(2,2,2)
plot(spm_dir_norm(a)), hold on, plot(spm_softmax(spm_psi(a))), hold off
title ('high Dirichlet counts')
legend({'expectation','variational'})

%% and the difference between the log of an expected Dirichlet
% distribution and the expected log
%--------------------------------------------------------------------------
K     = 4;
for i = 1:64
    a    = ones(K,1);
    a(1) = i/32;
    a0   = sum(a);

    ELOG(i) = psi(a(1)) - psi(a0);
    LOGE(i) = log(a(1)/a0);
    P(i)    = a(1);
end

plot(P,LOGE,P,ELOG)
legend({'log of expectation','expectation of log'})









